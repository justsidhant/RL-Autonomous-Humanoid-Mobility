{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d1294b4",
      "metadata": {
        "id": "9d1294b4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from collections import namedtuple\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import math\n",
        "import argparse\n",
        "from itertools import count\n",
        "import gym\n",
        "import scipy.optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d8d0796",
      "metadata": {
        "id": "5d8d0796"
      },
      "outputs": [],
      "source": [
        "def conjugate_gradients(Avp, b, nsteps, residual_tol=1e-10):\n",
        "    x = torch.zeros(b.size())\n",
        "    r = b - Avp(x)\n",
        "    p = r\n",
        "    rdotr = torch.dot(r, r)\n",
        "\n",
        "    for i in range(nsteps):\n",
        "        _Avp = Avp(p)\n",
        "        alpha = rdotr / torch.dot(p, _Avp)\n",
        "        x += alpha * p\n",
        "        r -= alpha * _Avp\n",
        "        new_rdotr = torch.dot(r, r)\n",
        "        betta = new_rdotr / rdotr\n",
        "        p = r + betta * p\n",
        "        rdotr = new_rdotr\n",
        "        if rdotr < residual_tol:\n",
        "            break\n",
        "    return x\n",
        "\n",
        "def flat_grad_from(net, grad_grad=False):\n",
        "    grads = []\n",
        "    for param in net.parameters():\n",
        "        if grad_grad:\n",
        "            grads.append(param.grad.grad.view(-1))\n",
        "        else:\n",
        "            grads.append(param.grad.view(-1))\n",
        "\n",
        "    flat_grad = torch.cat(grads)\n",
        "    return flat_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4532eabf",
      "metadata": {
        "id": "4532eabf"
      },
      "outputs": [],
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs):\n",
        "        super(Policy, self).__init__()\n",
        "        self.affine1 = nn.Linear(num_inputs, 64)\n",
        "        self.affine2 = nn.Linear(64, 64)\n",
        "\n",
        "        self.action_mean = nn.Linear(64, num_outputs)\n",
        "        self.action_mean.weight.data.mul_(0.1)\n",
        "        self.action_mean.bias.data.mul_(0.0)\n",
        "\n",
        "        self.action_log_std = nn.Parameter(torch.zeros(1, num_outputs))\n",
        "\n",
        "        self.saved_actions = []\n",
        "        self.rewards = []\n",
        "        self.final_value = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.tanh(self.affine1(x))\n",
        "        x = torch.tanh(self.affine2(x))\n",
        "\n",
        "        action_mean = self.action_mean(x)\n",
        "        action_log_std = self.action_log_std.expand_as(action_mean)\n",
        "        action_std = torch.exp(action_log_std)\n",
        "\n",
        "        return action_mean, action_log_std, action_std\n",
        "\n",
        "\n",
        "class Value(nn.Module):\n",
        "    def __init__(self, num_inputs):\n",
        "        super(Value, self).__init__()\n",
        "        self.affine1 = nn.Linear(num_inputs, 64)\n",
        "        self.affine2 = nn.Linear(64, 64)\n",
        "        self.value_head = nn.Linear(64, 1)\n",
        "        self.value_head.weight.data.mul_(0.1)\n",
        "        self.value_head.bias.data.mul_(0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.tanh(self.affine1(x))\n",
        "        x = torch.tanh(self.affine2(x))\n",
        "\n",
        "        state_values = self.value_head(x)\n",
        "        return state_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99dbc16f",
      "metadata": {
        "id": "99dbc16f"
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition', ('state', 'action', 'mask', 'next_state',\n",
        "                                       'reward'))\n",
        "\n",
        "\n",
        "class Memory(object):\n",
        "    def __init__(self):\n",
        "        self.memory = []\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self):\n",
        "        return Transition(*zip(*self.memory))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef175a4a",
      "metadata": {
        "id": "ef175a4a"
      },
      "outputs": [],
      "source": [
        "class RunningStat(object):\n",
        "    def __init__(self, shape):\n",
        "        self._n = 0\n",
        "        self._M = np.zeros(shape)\n",
        "        self._S = np.zeros(shape)\n",
        "\n",
        "    def push(self, x):\n",
        "        x = np.asarray(x)\n",
        "        assert x.shape == self._M.shape\n",
        "        self._n += 1\n",
        "        if self._n == 1:\n",
        "            self._M[...] = x\n",
        "        else:\n",
        "            oldM = self._M.copy()\n",
        "            self._M[...] = oldM + (x - oldM) / self._n\n",
        "            self._S[...] = self._S + (x - oldM) * (x - self._M)\n",
        "\n",
        "    @property\n",
        "    def n(self):\n",
        "        return self._n\n",
        "\n",
        "    @property\n",
        "    def mean(self):\n",
        "        return self._M\n",
        "\n",
        "    @property\n",
        "    def var(self):\n",
        "        return self._S / (self._n - 1) if self._n > 1 else np.square(self._M)\n",
        "\n",
        "    @property\n",
        "    def std(self):\n",
        "        return np.sqrt(self.var)\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        return self._M.shape\n",
        "\n",
        "\n",
        "class ZFilter:\n",
        "    \"\"\"\n",
        "    y = (x-mean)/std\n",
        "    using running estimates of mean,std\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, shape, demean=True, destd=True, clip=10.0):\n",
        "        self.demean = demean\n",
        "        self.destd = destd\n",
        "        self.clip = clip\n",
        "\n",
        "        self.rs = RunningStat(shape)\n",
        "\n",
        "    def __call__(self, x, update=True):\n",
        "        if update: self.rs.push(x)\n",
        "        if self.demean:\n",
        "            x = x - self.rs.mean\n",
        "        if self.destd:\n",
        "            x = x / (self.rs.std + 1e-8)\n",
        "        if self.clip:\n",
        "            x = np.clip(x, -self.clip, self.clip)\n",
        "        return x\n",
        "\n",
        "    def output_shape(self, input_space):\n",
        "        return input_space.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1f5b3f1",
      "metadata": {
        "id": "f1f5b3f1"
      },
      "outputs": [],
      "source": [
        "def normal_entropy(std):\n",
        "    var = std.pow(2)\n",
        "    entropy = 0.5 + 0.5 * torch.log(2 * var * math.pi)\n",
        "    return entropy.sum(1, keepdim=True)\n",
        "\n",
        "\n",
        "def normal_log_density(x, mean, log_std, std):\n",
        "    var = std.pow(2)\n",
        "    log_density = -(x - mean).pow(2) / (\n",
        "        2 * var) - 0.5 * math.log(2 * math.pi) - log_std\n",
        "    return log_density.sum(1, keepdim=True)\n",
        "\n",
        "\n",
        "def get_flat_params_from(model):\n",
        "    params = []\n",
        "    for param in model.parameters():\n",
        "        params.append(param.data.view(-1))\n",
        "\n",
        "    flat_params = torch.cat(params)\n",
        "    return flat_params\n",
        "\n",
        "\n",
        "def set_flat_params_to(model, flat_params):\n",
        "    prev_ind = 0\n",
        "    for param in model.parameters():\n",
        "        flat_size = int(np.prod(list(param.size())))\n",
        "        param.data.copy_(\n",
        "            flat_params[prev_ind:prev_ind + flat_size].view(param.size()))\n",
        "        prev_ind += flat_size\n",
        "\n",
        "\n",
        "def get_flat_grad_from(net, grad_grad=False):\n",
        "    grads = []\n",
        "    for param in net.parameters():\n",
        "        if grad_grad:\n",
        "            grads.append(param.grad.grad.view(-1))\n",
        "        else:\n",
        "            grads.append(param.grad.view(-1))\n",
        "\n",
        "    flat_grad = torch.cat(grads)\n",
        "    return flat_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8cdbf9c",
      "metadata": {
        "id": "a8cdbf9c"
      },
      "outputs": [],
      "source": [
        "def conjugate_gradients(Avp, b, nsteps, residual_tol=1e-10):\n",
        "    x = torch.zeros(b.size())\n",
        "    r = b.clone()\n",
        "    p = b.clone()\n",
        "    rdotr = torch.dot(r, r)\n",
        "    for i in range(nsteps):\n",
        "        _Avp = Avp(p)\n",
        "        alpha = rdotr / torch.dot(p, _Avp)\n",
        "        x += alpha * p\n",
        "        r -= alpha * _Avp\n",
        "        new_rdotr = torch.dot(r, r)\n",
        "        betta = new_rdotr / rdotr\n",
        "        p = r + betta * p\n",
        "        rdotr = new_rdotr\n",
        "        if rdotr < residual_tol:\n",
        "            break\n",
        "    return x\n",
        "\n",
        "\n",
        "def linesearch(model,\n",
        "               f,\n",
        "               x,\n",
        "               fullstep,\n",
        "               expected_improve_rate,\n",
        "               max_backtracks=10,\n",
        "               accept_ratio=.1):\n",
        "    fval = f(True).data\n",
        "    print(\"fval before\", fval.item())\n",
        "    for (_n_backtracks, stepfrac) in enumerate(.5**np.arange(max_backtracks)):\n",
        "        xnew = x + stepfrac * fullstep\n",
        "        set_flat_params_to(model, xnew)\n",
        "        newfval = f(True).data\n",
        "        actual_improve = fval - newfval\n",
        "        expected_improve = expected_improve_rate * stepfrac\n",
        "        ratio = actual_improve / expected_improve\n",
        "        print(\"a/e/r\", actual_improve.item(), expected_improve.item(), ratio.item())\n",
        "\n",
        "        if ratio.item() > accept_ratio and actual_improve.item() > 0:\n",
        "            print(\"fval after\", newfval.item())\n",
        "            return True, xnew\n",
        "    return False, x\n",
        "\n",
        "\n",
        "def trpo_step(model, get_loss, get_kl, max_kl, damping):\n",
        "    loss = get_loss()\n",
        "    grads = torch.autograd.grad(loss, model.parameters())\n",
        "    loss_grad = torch.cat([grad.view(-1) for grad in grads]).data\n",
        "\n",
        "    def Fvp(v):\n",
        "        kl = get_kl()\n",
        "        kl = kl.mean()\n",
        "\n",
        "        grads = torch.autograd.grad(kl, model.parameters(), create_graph=True)\n",
        "        flat_grad_kl = torch.cat([grad.view(-1) for grad in grads])\n",
        "\n",
        "        kl_v = (flat_grad_kl * Variable(v)).sum()\n",
        "        grads = torch.autograd.grad(kl_v, model.parameters())\n",
        "        flat_grad_grad_kl = torch.cat([grad.contiguous().view(-1) for grad in grads]).data\n",
        "\n",
        "        return flat_grad_grad_kl + v * damping\n",
        "\n",
        "    stepdir = conjugate_gradients(Fvp, -loss_grad, 10)\n",
        "\n",
        "    shs = 0.5 * (stepdir * Fvp(stepdir)).sum(0, keepdim=True)\n",
        "\n",
        "    lm = torch.sqrt(shs / max_kl)\n",
        "    fullstep = stepdir / lm[0]\n",
        "\n",
        "    neggdotstepdir = (-loss_grad * stepdir).sum(0, keepdim=True)\n",
        "    print((\"lagrange multiplier:\", lm[0], \"grad_norm:\", loss_grad.norm()))\n",
        "\n",
        "    prev_params = get_flat_params_from(model)\n",
        "    success, new_params = linesearch(model, get_loss, prev_params, fullstep,\n",
        "                                     neggdotstepdir / lm[0])\n",
        "    set_flat_params_to(model, new_params)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32a56769",
      "metadata": {
        "id": "32a56769"
      },
      "outputs": [],
      "source": [
        "torch.utils.backcompat.broadcast_warning.enabled = True\n",
        "torch.utils.backcompat.keepdim_warning.enabled = True\n",
        "\n",
        "torch.set_default_tensor_type('torch.DoubleTensor')\n",
        "\n",
        "# parser = argparse.ArgumentParser(description='PyTorch actor-critic example')\n",
        "# parser.add_argument('--gamma', type=float, default=0.995, metavar='G',\n",
        "#                     help='discount factor (default: 0.995)')\n",
        "# parser.add_argument('--env-name', default=\"Reacher-v1\", metavar='G',\n",
        "#                     help='name of the environment to run')\n",
        "# parser.add_argument('--tau', type=float, default=0.97, metavar='G',\n",
        "#                     help='gae (default: 0.97)')\n",
        "# parser.add_argument('--l2-reg', type=float, default=1e-3, metavar='G',\n",
        "#                     help='l2 regularization regression (default: 1e-3)')\n",
        "# parser.add_argument('--max-kl', type=float, default=1e-2, metavar='G',\n",
        "#                     help='max kl value (default: 1e-2)')\n",
        "# parser.add_argument('--damping', type=float, default=1e-1, metavar='G',\n",
        "#                     help='damping (default: 1e-1)')\n",
        "# parser.add_argument('--seed', type=int, default=543, metavar='N',\n",
        "#                     help='random seed (default: 1)')\n",
        "# parser.add_argument('--batch-size', type=int, default=15000, metavar='N',\n",
        "#                     help='random seed (default: 1)')\n",
        "# parser.add_argument('--render', action='store_true',\n",
        "#                     help='render the environment')\n",
        "# parser.add_argument('--log-interval', type=int, default=1, metavar='N',\n",
        "#                     help='interval between training status logs (default: 10)')\n",
        "# args = parser.parse_args()\n",
        "\n",
        "env_name = \"Humanoid-v4\"\n",
        "gamma = 0.995\n",
        "tau = 0.97\n",
        "l2_reg = 1e-3\n",
        "max_kl = 1e-2\n",
        "damping = 1e-1\n",
        "seed = 543\n",
        "batch_size = 50000\n",
        "log_interval = 1\n",
        "render = True\n",
        "\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "num_inputs = env.observation_space.shape[0]\n",
        "num_actions = env.action_space.shape[0]\n",
        "\n",
        "# env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "policy_net = Policy(num_inputs, num_actions)\n",
        "value_net = Value(num_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "372f5e40",
      "metadata": {
        "id": "372f5e40",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "policy_net.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dab430e",
      "metadata": {
        "id": "5dab430e"
      },
      "outputs": [],
      "source": [
        "def select_action(state):\n",
        "    state = torch.from_numpy(state).unsqueeze(0)\n",
        "    action_mean, _, action_std = policy_net(Variable(state))\n",
        "    action = torch.normal(action_mean, action_std)\n",
        "    return action\n",
        "\n",
        "def update_params(batch):\n",
        "    rewards = torch.Tensor(batch.reward)\n",
        "    masks = torch.Tensor(batch.mask)\n",
        "    actions = torch.Tensor(np.concatenate(batch.action, 0))\n",
        "    states = torch.Tensor(batch.state)\n",
        "    values = value_net(Variable(states))\n",
        "\n",
        "    returns = torch.Tensor(actions.size(0),1)\n",
        "    deltas = torch.Tensor(actions.size(0),1)\n",
        "    advantages = torch.Tensor(actions.size(0),1)\n",
        "\n",
        "    prev_return = 0\n",
        "    prev_value = 0\n",
        "    prev_advantage = 0\n",
        "    for i in reversed(range(rewards.size(0))):\n",
        "        returns[i] = rewards[i] + gamma * prev_return * masks[i]\n",
        "        deltas[i] = rewards[i] + gamma * prev_value * masks[i] - values.data[i]\n",
        "        advantages[i] = deltas[i] + gamma * tau * prev_advantage * masks[i]\n",
        "\n",
        "        prev_return = returns[i, 0]\n",
        "        prev_value = values.data[i, 0]\n",
        "        prev_advantage = advantages[i, 0]\n",
        "\n",
        "    targets = Variable(returns)\n",
        "\n",
        "    # Original code uses the same LBFGS to optimize the value loss\n",
        "    def get_value_loss(flat_params):\n",
        "        set_flat_params_to(value_net, torch.Tensor(flat_params))\n",
        "        for param in value_net.parameters():\n",
        "            if param.grad is not None:\n",
        "                param.grad.data.fill_(0)\n",
        "\n",
        "        values_ = value_net(Variable(states))\n",
        "\n",
        "        value_loss = (values_ - targets).pow(2).mean()\n",
        "\n",
        "        # weight decay\n",
        "        for param in value_net.parameters():\n",
        "            value_loss += param.pow(2).sum() * l2_reg\n",
        "        value_loss.backward()\n",
        "        return (value_loss.data.double().numpy(), get_flat_grad_from(value_net).data.double().numpy())\n",
        "\n",
        "    flat_params, _, opt_info = scipy.optimize.fmin_l_bfgs_b(get_value_loss, get_flat_params_from(value_net).double().numpy(), maxiter=25)\n",
        "    set_flat_params_to(value_net, torch.Tensor(flat_params))\n",
        "\n",
        "    advantages = (advantages - advantages.mean()) / advantages.std()\n",
        "\n",
        "    action_means, action_log_stds, action_stds = policy_net(Variable(states))\n",
        "    fixed_log_prob = normal_log_density(Variable(actions), action_means, action_log_stds, action_stds).data.clone()\n",
        "\n",
        "    def get_loss(volatile=False):\n",
        "        if volatile:\n",
        "            with torch.no_grad():\n",
        "                action_means, action_log_stds, action_stds = policy_net(Variable(states))\n",
        "        else:\n",
        "            action_means, action_log_stds, action_stds = policy_net(Variable(states))\n",
        "\n",
        "        log_prob = normal_log_density(Variable(actions), action_means, action_log_stds, action_stds)\n",
        "        action_loss = -Variable(advantages) * torch.exp(log_prob - Variable(fixed_log_prob))\n",
        "        return action_loss.mean()\n",
        "\n",
        "\n",
        "    def get_kl():\n",
        "        mean1, log_std1, std1 = policy_net(Variable(states))\n",
        "\n",
        "        mean0 = Variable(mean1.data)\n",
        "        log_std0 = Variable(log_std1.data)\n",
        "        std0 = Variable(std1.data)\n",
        "        kl = log_std1 - log_std0 + (std0.pow(2) + (mean0 - mean1).pow(2)) / (2.0 * std1.pow(2)) - 0.5\n",
        "        return kl.sum(1, keepdim=True)\n",
        "\n",
        "    trpo_step(policy_net, get_loss, get_kl, max_kl, damping)\n",
        "\n",
        "running_state = ZFilter((num_inputs,), clip=5)\n",
        "running_reward = ZFilter((1,), demean=False, clip=10)\n",
        "episodic_reward=[]\n",
        "for i_episode in range(500):\n",
        "    memory = Memory()\n",
        "\n",
        "    num_steps = 0\n",
        "    reward_batch = 0\n",
        "    num_episodes = 0\n",
        "    while num_steps < batch_size:\n",
        "        state = env.reset()\n",
        "        state = running_state(state[0])\n",
        "\n",
        "        reward_sum = 0\n",
        "        for t in range(10000): # Don't infinite loop while learning\n",
        "            action = select_action(state)\n",
        "            action = action.data[0].numpy()\n",
        "            next_state, reward, done, _,_= env.step(action)\n",
        "            reward_sum += reward\n",
        "\n",
        "            next_state = running_state(next_state)\n",
        "\n",
        "            mask = 1\n",
        "            if done:\n",
        "                mask = 0\n",
        "\n",
        "            memory.push(state, np.array([action]), mask, next_state, reward)\n",
        "\n",
        "            if render:\n",
        "                env.render()\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            state = next_state\n",
        "        num_steps += (t-1)\n",
        "        num_episodes += 1\n",
        "        reward_batch += reward_sum\n",
        "\n",
        "    reward_batch /= num_episodes\n",
        "    batch = memory.sample()\n",
        "    update_params(batch)\n",
        "    episodic_reward.append(reward_batch)\n",
        "    if i_episode % log_interval == 0:\n",
        "        print('Episode {}\\tLast reward: {}\\tAverage reward {:.2f}'.format(\n",
        "            i_episode, reward_sum, reward_batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "053bea7b",
      "metadata": {
        "id": "053bea7b"
      },
      "outputs": [],
      "source": [
        "path = \"policy_net.pt\"\n",
        "torch.save(policy_net.state_dict(),path)\n",
        "path = \"value_net.pt\"\n",
        "torch.save(value_net.state_dict(),path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "114ec2ae",
      "metadata": {
        "id": "114ec2ae"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "#filename=open('RL_Project_Reward.txt','wb')\n",
        "#pickle.dump(episodic_reward,filename)\n",
        "#filename.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "062db223",
      "metadata": {
        "id": "062db223"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(500),episodic_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46864d20",
      "metadata": {
        "id": "46864d20"
      },
      "outputs": [],
      "source": [
        "torch.utils.backcompat.broadcast_warning.enabled = True\n",
        "torch.utils.backcompat.keepdim_warning.enabled = True\n",
        "\n",
        "torch.set_default_tensor_type('torch.DoubleTensor')\n",
        "\n",
        "# parser = argparse.ArgumentParser(description='PyTorch actor-critic example')\n",
        "# parser.add_argument('--gamma', type=float, default=0.995, metavar='G',\n",
        "#                     help='discount factor (default: 0.995)')\n",
        "# parser.add_argument('--env-name', default=\"Reacher-v1\", metavar='G',\n",
        "#                     help='name of the environment to run')\n",
        "# parser.add_argument('--tau', type=float, default=0.97, metavar='G',\n",
        "#                     help='gae (default: 0.97)')\n",
        "# parser.add_argument('--l2-reg', type=float, default=1e-3, metavar='G',\n",
        "#                     help='l2 regularization regression (default: 1e-3)')\n",
        "# parser.add_argument('--max-kl', type=float, default=1e-2, metavar='G',\n",
        "#                     help='max kl value (default: 1e-2)')\n",
        "# parser.add_argument('--damping', type=float, default=1e-1, metavar='G',\n",
        "#                     help='damping (default: 1e-1)')\n",
        "# parser.add_argument('--seed', type=int, default=543, metavar='N',\n",
        "#                     help='random seed (default: 1)')\n",
        "# parser.add_argument('--batch-size', type=int, default=15000, metavar='N',\n",
        "#                     help='random seed (default: 1)')\n",
        "# parser.add_argument('--render', action='store_true',\n",
        "#                     help='render the environment')\n",
        "# parser.add_argument('--log-interval', type=int, default=1, metavar='N',\n",
        "#                     help='interval between training status logs (default: 10)')\n",
        "# args = parser.parse_args()\n",
        "\n",
        "env_name = \"Humanoid-v4\"\n",
        "gamma = 0.995\n",
        "tau = 0.97\n",
        "l2_reg = 1e-3\n",
        "max_kl = 1e-2\n",
        "damping = 1e-1\n",
        "seed = 543\n",
        "batch_size = 50000\n",
        "log_interval = 1\n",
        "render = True\n",
        "\n",
        "\n",
        "env = gym.make(env_name,render_mode=\"human\")\n",
        "\n",
        "num_inputs = env.observation_space.shape[0]\n",
        "num_actions = env.action_space.shape[0]\n",
        "\n",
        "# env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "policy_net = Policy(num_inputs, num_actions)\n",
        "value_net = Value(num_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c53163b9",
      "metadata": {
        "id": "c53163b9"
      },
      "outputs": [],
      "source": [
        "#testing\n",
        "policy_net.load_state_dict(torch.load('policy_net.pt'),strict=False)\n",
        "value_net.load_state_dict(torch.load('value_net.pt'),strict=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "963bbec0",
      "metadata": {
        "id": "963bbec0"
      },
      "outputs": [],
      "source": [
        "def select_action(state):\n",
        "    state = torch.from_numpy(state).unsqueeze(0)\n",
        "    action_mean, _, action_std = policy_net(Variable(state))\n",
        "    action = torch.normal(action_mean, action_std)\n",
        "    return action\n",
        "\n",
        "def update_params(batch):\n",
        "    rewards = torch.Tensor(batch.reward)\n",
        "    masks = torch.Tensor(batch.mask)\n",
        "    actions = torch.Tensor(np.concatenate(batch.action, 0))\n",
        "    states = torch.Tensor(batch.state)\n",
        "    values = value_net(Variable(states))\n",
        "\n",
        "    returns = torch.Tensor(actions.size(0),1)\n",
        "    deltas = torch.Tensor(actions.size(0),1)\n",
        "    advantages = torch.Tensor(actions.size(0),1)\n",
        "\n",
        "    prev_return = 0\n",
        "    prev_value = 0\n",
        "    prev_advantage = 0\n",
        "    for i in reversed(range(rewards.size(0))):\n",
        "        returns[i] = rewards[i] + gamma * prev_return * masks[i]\n",
        "        deltas[i] = rewards[i] + gamma * prev_value * masks[i] - values.data[i]\n",
        "        advantages[i] = deltas[i] + gamma * tau * prev_advantage * masks[i]\n",
        "\n",
        "        prev_return = returns[i, 0]\n",
        "        prev_value = values.data[i, 0]\n",
        "        prev_advantage = advantages[i, 0]\n",
        "\n",
        "    targets = Variable(returns)\n",
        "\n",
        "    # Original code uses the same LBFGS to optimize the value loss\n",
        "    def get_value_loss(flat_params):\n",
        "        set_flat_params_to(value_net, torch.Tensor(flat_params))\n",
        "        for param in value_net.parameters():\n",
        "            if param.grad is not None:\n",
        "                param.grad.data.fill_(0)\n",
        "\n",
        "        values_ = value_net(Variable(states))\n",
        "\n",
        "        value_loss = (values_ - targets).pow(2).mean()\n",
        "\n",
        "        # weight decay\n",
        "        for param in value_net.parameters():\n",
        "            value_loss += param.pow(2).sum() * l2_reg\n",
        "        value_loss.backward()\n",
        "        return (value_loss.data.double().numpy(), get_flat_grad_from(value_net).data.double().numpy())\n",
        "\n",
        "    flat_params, _, opt_info = scipy.optimize.fmin_l_bfgs_b(get_value_loss, get_flat_params_from(value_net).double().numpy(), maxiter=25)\n",
        "    set_flat_params_to(value_net, torch.Tensor(flat_params))\n",
        "\n",
        "    advantages = (advantages - advantages.mean()) / advantages.std()\n",
        "\n",
        "    action_means, action_log_stds, action_stds = policy_net(Variable(states))\n",
        "    fixed_log_prob = normal_log_density(Variable(actions), action_means, action_log_stds, action_stds).data.clone()\n",
        "\n",
        "    def get_loss(volatile=False):\n",
        "        if volatile:\n",
        "            with torch.no_grad():\n",
        "                action_means, action_log_stds, action_stds = policy_net(Variable(states))\n",
        "        else:\n",
        "            action_means, action_log_stds, action_stds = policy_net(Variable(states))\n",
        "\n",
        "        log_prob = normal_log_density(Variable(actions), action_means, action_log_stds, action_stds)\n",
        "        action_loss = -Variable(advantages) * torch.exp(log_prob - Variable(fixed_log_prob))\n",
        "        return action_loss.mean()\n",
        "\n",
        "\n",
        "    def get_kl():\n",
        "        mean1, log_std1, std1 = policy_net(Variable(states))\n",
        "\n",
        "        mean0 = Variable(mean1.data)\n",
        "        log_std0 = Variable(log_std1.data)\n",
        "        std0 = Variable(std1.data)\n",
        "        kl = log_std1 - log_std0 + (std0.pow(2) + (mean0 - mean1).pow(2)) / (2.0 * std1.pow(2)) - 0.5\n",
        "        return kl.sum(1, keepdim=True)\n",
        "\n",
        "    trpo_step(policy_net, get_loss, get_kl, max_kl, damping)\n",
        "\n",
        "running_state = ZFilter((num_inputs,), clip=5)\n",
        "running_reward = ZFilter((1,), demean=False, clip=10)\n",
        "episodic_reward=[]\n",
        "for i_episode in range(50):\n",
        "    memory = Memory()\n",
        "\n",
        "    num_steps = 0\n",
        "    reward_batch = 0\n",
        "    num_episodes = 0\n",
        "    while num_steps < batch_size:\n",
        "        state = env.reset()\n",
        "        state = running_state(state[0])\n",
        "\n",
        "        reward_sum = 0\n",
        "        for t in range(10000): # Don't infinite loop while learning\n",
        "            action = select_action(state)\n",
        "            action = action.data[0].numpy()\n",
        "            next_state, reward, done, _,_= env.step(action)\n",
        "            reward_sum += reward\n",
        "\n",
        "            next_state = running_state(next_state)\n",
        "\n",
        "            mask = 1\n",
        "            if done:\n",
        "                mask = 0\n",
        "\n",
        "            memory.push(state, np.array([action]), mask, next_state, reward)\n",
        "\n",
        "            if render:\n",
        "                env.render()\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            state = next_state\n",
        "        num_steps += (t-1)\n",
        "        num_episodes += 1\n",
        "        reward_batch += reward_sum\n",
        "\n",
        "    reward_batch /= num_episodes\n",
        "    batch = memory.sample()\n",
        "    update_params(batch)\n",
        "    episodic_reward.append(reward_batch)\n",
        "    if i_episode % log_interval == 0:\n",
        "        print('Episode {}\\tLast reward: {}\\tAverage reward {:.2f}'.format(\n",
        "            i_episode, reward_sum, reward_batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b50a399e",
      "metadata": {
        "id": "b50a399e"
      },
      "outputs": [],
      "source": [
        "policy_net.state_dict()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}